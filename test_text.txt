Neuromorphic computing (NMC) is increasingly viewed as a low-power alternative to conventional von Neumann architectures such as central processing units (CPUs) and graphics processing units (GPUs), however the computational value proposition has been difficult to define precisely.

Here, we explain how NMC should be seen as general-purpose and programmable even though it differs considerably from a conventional stored-program architecture. We show that the time and space scaling of NMC is equivalent to that of a theoretically infinite processor conventional system, however the energy scaling is significantly different. Specifically, the energy of conventional systems scales with absolute algorithm work, whereas the energy of neuromorphic systems scales with the derivative of algorithm state. The unique characteristics of NMC architectures make it well suited for different classes of algorithms than conventional multi-core systems like GPUs that have been optimized for dense numerical applications such as linear algebra. In contrast, the unique characteristics of NMC make it ideally suited for scalable and sparse algorithms whose activity is proportional to an objective function, such as iterative optimization and large-scale sampling (e.g., Monte Carlo).

IIntroduction
Neuromorphic computing (‘NMC’) hardware is increasingly available and can be implemented at near brain-like scales [1]. One implication of the success of implementing large-scale NMC systems is that it is increasingly evident that the biggest challenge facing the neuromorphic field, at least in the near-term, is the identification of which applications are well suited for its use. While scalable NMC platforms originally targeted large-scale brain simulations [2, 3], the growing need for low-power solutions at both the edge and in high-performance computing systems has increased interest in NMC to address energy challenges in computation broadly.

Key to their value proposition is that today’s digital NMC systems are generically programmable—aside from some constraints of fan-in/fan-out, these systems can implement arbitrary computational graphs. Conceptually, this compatibility includes any algorithm that can be formulated as a threshold gate (TG) circuit, artificial neural network (ANNs), or any other more complex ensemble of neurons. As the ability to implement TGs confers Turing completeness [4], and ANNs confer universal function approximation [5], we can deduce that NMC is both universal in terms of algorithm compatibility and its ability to approximate functions.

Given this universality, the relevant question of NMC is not “can neuromorphic solve this task?”, but rather “should NMC be used to solve this task?”. While NMC is by definition general purpose in potential, based on the ‘No Free Lunch’ theorem as applied to computer architectures, it should be expected that it will be better at some tasks compared to others [6]. The value proposition of NMC is increasingly important given the increased use of other specialized architectures, such as general-purpose graphics processing units (‘GP-GPUs’, or simply ‘GPUs’).

This value proposition has been further complicated by the fact that the NMC field itself spans a number of different timescales and levels of technology readiness [7, 8, 9]. Unlike conventional architectures that are commercial today and emerging technologies such as quantum computing that will likely remain research platforms for the foreseeable future, neuromorphic research includes technologies that are near-ready for widespread adoption now as well as exploration of novel approaches that are many years out.

While there has been some increased attention to theoretical models for physical computing [10], there have been only a few efforts looking at the theoretical implications of scalable NMC platforms [11, 12, 13]. Despite the lack of a formal theoretical model, there has been considerable progress in recent years in identifying classes of problems that are well suited for addressing with neuromorphic algorithms. The goal of this analysis is to take a step back and look at how NMC architectures differ from Von Neumann architectures and systematically explore what classes of algorithms are well suited for neuromorphic computing.

This paper describes how NMC can be viewed as a class of specialized general-purpose architectures that is complementary to GPUs and other types of linear algebra accelerators that have become widespread in computing today. Figure 1 illustrates the notional hypothesis of this analysis: like GPUs, there exist several classes of computations that NMC excels at relative to conventional processors, and moreover that this is a distinct set of applications than what modern accelerators have targeted. Through the next few sections, we will discuss how NMC architecture differ from the conventional Von Neumann approach and we will show how these differences have direct impact on what types of computations NMC excels at relative to more conventional architectural approaches.

Refer to caption
Figure 1:NMC hardware is GPU-like in generality, but shows advantageous capabilities in a different set of tasks. This paper explores the classes of computation for which NMC shows preferential advantages
IIDefining Neuromorphic Architectures in the Context of Von Neumann
NMC is typically described as a non-Von Neumann architecture, but rarely is it specified what type of architecture that it is. Here, we will discuss what the implications of being non-Von Neumann are and how this affects how we should view today’s NMC platforms. However, first it is useful to briefly characterize what a Von Neumann architecture is and why it is so powerful.

II-AStrengths and weaknesses of Von Neumann
At a very simple level, the Von Neumann architecture refers to a stored-program architecture wherein a memory external to the processor includes the instructions that represent a program as well as the data that will be processed. At the lowest level, the processor has specialized logic such as arithmetic logic units (ALU) that consist of a spectrum of specialized low-level circuitry to perform specific operations. From those basic operations, programs can be constructed to implement more sophisticated calculations. For example, a simple ALU has dedicated hardware to implement basic binary arithmetic (e.g., addition, subtraction), comparison operations (e.g., greater than, less than), and logical operations (e.g., bitwise logical AND, XOR). A Von Neumann program is simply a series of instructions that consist of which hardware-level operation to use, where in memory to pull inputs from, and where to store the outputs.

Von Neumann architectures are ubiquitous today for good reason. By separating processing and memory, each aspect can be designed and improved independently of the other. For instance, a powerful general-purpose processor, like a CPU, can dedicate considerable resources to having a wide range of increasingly sophisticated calculations hardware accelerated, while a more specialized processor can prioritize a subset of operations (as a GPU does for linear algebra), and a Reduced Instruction Set Computing (RISC) architecture may only have a lightweight set of instructions to maximize efficiency. Similarly, the separation of memory has allowed industry to focus on increasing density and access speeds, and with 64-bit addressing, there is effectively no limit to program or data size. This separation has also led to an important feature that has further entrenched the architecture—a serial program written fifty years ago in principle can run on today’s hardware and vice versa. As a result, arguably this feature entrenched Von Neumann programs as the catalyst of the first Hardware Lottery [14].

The downside of Von Neumann is that scaling to larger systems with more memory and more powerful compute elements physically results in moving computation further away from memory. Stated differently, a bigger memory or a bigger processor requires that information—both data and the program itself—be moved longer distances. For this reason, the vast majority of the energy cost of computers today is in memory accesses [15], and much of modern computer architecture research focuses on this challenge [16]. This is one reason why Moore’s Law was so critical, so long as transistor sizes were getting smaller, more memory could easily be placed nearer to the processing which effectively bounded the time and energy costs of having to go off chip.

II-BFormal model definitions for neuromorphic computing
While the NMC landscape continues to evolve, it is useful for the remainder of this analysis to have a concrete theoretical model to refer to. Here, we define a concrete model of NMC and in the following sections we will analyze the distinctions of this model from Von Neumann and the implications of its hardware realization.

II-B1Neurons and Synapses
First, we provide generic definitions of the two key classes of primitives in a NMC system, neurons and synapses.

Inherent in the graph-based description of NMC programs described above is the implication that NMC is intrinsically a parallel architecture. While details differ across NMC platforms, algorithmically NMC is typically be viewed as having every neuron acting independently and asynchronously from one another. It should be emphasized how different this extremely parallel neural programming paradigm is from conventional parallel architectures. The underlying serial nature of Von Neumann is fundamental in most parallel architectures—adding more cores to a computation typically amounts to distributing a set of primarily serial workloads across processors and carefully coordinating the communication of information to align those workloads.

Parallel computing introduces a whole set of unique challenges at both the algorithm and architectural level, and one significant simplification that has led to significant efficiencies is to focus on “single-instruction, multiple data”, or SIMD, approaches that shape algorithms to target a number of very powerful compute cores with a set of common instructions that are applied to a large volume of data. For this reason, many parallel architectures, such as GPUs, are SIMD. SIMD-based architectures clearly are powerful with appropriate algorithms, such as linear algebra applications where the required mathematical operations and memory accesses are highly structured. The downside of SIMD is that if an application lacks structure, such as Monte Carlo simulations with highly divergent trajectories, the uniformity of SIMD becomes a drawback. It is in this heterogeneous setting, which can be termed multiple instruction, multiple data (MIMD), that we suggest that NMC may thrive. NMC effectively provides a unique path to an MIMD architecture—since every calculation is implemented in its own population of neurons, there is no reason that these calculations need to be the same. In effect, if a complex diverse computation is going to be spread across a large population of neurons, there is no reason not to make the neural implementation match that diversity and complexity.

The flip side of spreading out a computation in this manner is that communication costs, yet again, can become problematic. The brain’s solution to this, which has been adopted in most NMC systems, is to use spiking to minimize the costs of communication. Spiking refers to the event-driven communication of neurons, whereby a neuron only communicates if its inputs satisfy some condition, such as crossing a threshold. In the brain, this is an all-or-none single-bit—transmit a 1 or no transmission at all. In today’s NMC platforms the spike may consist of more information, but they are always event-driven (see section III-C) and almost always target-agnostic (i.e., all targets receive the same spike). This creates a data-dependent cost that is not present in conventional computation. If 0s are effectively free in communication and computation, sparsity becomes a particularly important feature of algorithm design.

An additional element introduced by the event-driven nature of spiking and the extreme parallelism of neural circuits is that time becomes important. Time is not really present in a serial Von Neumann setting beyond the ordering of instructions, and more often than not time is a challenge to be overcome in parallel computing through barriers and blocking communication. Rarely is time used as part of the computation. In contrast, in the brain when something happens is often as important as what happens. Increasingly, NMC algorithms have been shown to be able to use timing to perform computations efficiently, particularly in the implementation of graph algorithms [21, 12, 22].


Figure 2:Notional landscape of parallel computing approaches. For applications that are relatively homogeneous in their computational graph (e.g., the structured linear algebra of conventional neural networks), SIMD and dataflow architectures are effective, whereas for heterogeneous applications an MIMD or neuromorphic approach should be more effective.
The MIMD–SIMD difference of neuromorphic not only distinguishes it from GPUs, but it similarly separates it from many alternative in-memory computing architectures, such as resistive memory crossbars and optical computing approaches. Many in-memory computing approaches realize benefits not only from integrating computation with memory, but also by aligning the physics of hardware with the desired computation. While some systems preserve some level of programmability, most of these architectures generally can be viewed as SIMD (Figure 2). Because these architectures are non-Von Neumann and emphasize local memory, they are also often referred to as neuromorphic, but it is critical to appreciate the distinction of these systems in terms of programmability.

III-CNeuromorphic processing is data-dependent and often non-deterministic
The final distinction that we will discuss here is the unpredictability of neural computation. Biological neural systems are often considered noisy when compared to engineered systems. The extent and implications of this noise as it relates to the brain is debated; biological systems certainly do not have the same level of precision as digital systems, however it is not clear that their activity is impacted by biophysical noise. If anything, the presence of noise in brains appears to be intentional; for instance, the primary source of noise in brains is stochastic synaptic vesicle release, which is a biophysical process whose probabilities are tightly controlled and appear to be genetically linked to different neuron types. Stated differently, the noise in real neural systems is likely a feature and not a limitation.

Because communication is event-driven and neurons behave asynchronously, computation effectively becomes event-driven as well. An individual neuron processes information as it arrives. The combined effect of intrinsic stochasticity, distributed computation, and event-driven communication means that neuromorphic computation can easily become non-deterministic and non-repetitive. Stated differently, the same computation may be performed by many different patterns of neural behavior. How ensembles of such neurons can be constructed to produce reliable and arbitrary computations is an open question in the theoretical neuroscience field [23, 24, 25]. Nevertheless, it is widely recognized that equivalent biological computations are considered stable across a diverse set of neuron circuits [26], and thus computation abstracted to a higher level than neurons [24] can presumably be reliable even in a formal sense, just as sampling algorithms can be proven to approximate fixed distributions [27].

For this reason, many neural algorithms to date have minimized the use of stochasticity and asynchronous communication in their design or have included it in a very deliberate manner [28, 29]. There are reasons to believe that ubiquitous stochasticity could provide fundamental improvements to algorithms [30], and it is very likely that the ability of neuromorphic to enable sampling alongside of memory and compute provides added computational value.

Similarly, learning represents a form of non-determinism in an NMC system that is challenging to evaluate rigorously. In a sense, learning is a property of an algorithm, and the ability of an architecture to permit learning simply broadens the class of suitable algorithms. At the same time, the ability to learn probably does yield complexity advantages in that an algorithms initial implementation costs can be amortized over a longer lifetime in the face of context drift [31].

While the non-determinism and adaptability of biological neural circuits makes neural computation richer and potentially more computationally powerful from a theoretical sense, this complexity also risks making NMC more challenging for those familiar with precise and sequential deterministic algorithms. For this reason, and given the relatively immature state of probabilistic and adaptive neural algorithms, we will not examine these architectural benefits further here but simply acknowledge them as fundamental differences between NMC and conventional approaches. As the next sections describe, the differences between NMC and conventional architectures can be analyzed at a more fundamental level and future work will demonstrate how these distinctions can amplify the effects of ubiquitous stochasticity and learning.

IVWhat neuromorphic is good at
From the above description, we can see that NMC is an non-stored program, MIMD architecture where the computation is fully distributed over processing elements. In this sense, it is similar to a distributed memory MIMD architecture. However unlike standard MPI-based distributed computing approaches, where each core is responsible for executing a thread with a communication strategy such as MPI responsible for aligning spatially disparate threads, neuromorphic algorithms must realize the full algorithm spatially and asynchronously. The question that presents itself is whether there is any fundamental algorithmic advantage to using such an architecture?

Here, we will consider three primary theoretical metrics: how an NMC system impacts the time (
T
), space (
C
), and energy (
E
) of an algorithm. Most models of parallel computation consider the implications of parallel processors sharing a memory (e.g., the PRAM model [32, 33]), but there is no memory access in a neuromorphic architecture. However, from a processor perspective many of the same frameworks apply. For instance, it is useful here to consider a neuromorphic algorithm fully expanded out as a directed acyclic graph (DAG), which we call 
G
 per definition II.4. Summarized results of complexity scaling, which will be derived in the following sections, are presented in Table II.

IV-AParallel analysis of neuromorphic algorithms
IV-A1Time
Often workloads on a parallel system are analyzed from the perspective of Amdahl’s Law [34], however due to our algorithm graph formulation, here we leverage instead Brent’s Theorem [35] which provides insights into parallel algorithm design. Stated differently, Amdahl’s law is suitable when the algorithm is fixed and the parallel system is a free variable, but in our case we explore when the algorithm (whether conventional or neuromorphic) is a free variable. Brent’s Theorem (which in its naive form is agnostic to hardware) can be stated as

max
⁡
(
T
inf
,
T
1
p
)
≤
T
p
≤
T
1
p
+
T
inf

